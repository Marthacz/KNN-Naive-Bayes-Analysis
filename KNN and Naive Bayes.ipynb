{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Martha Czernuszenko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes and KNN using scikit-learn\n",
    "\n",
    "From assignment: Implement the Naive Bayes and KNN classifier to classify patients as either having or not having diabetic retinopathy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>prescreen</th>\n",
       "      <th>ma2</th>\n",
       "      <th>ma3</th>\n",
       "      <th>ma4</th>\n",
       "      <th>ma5</th>\n",
       "      <th>ma6</th>\n",
       "      <th>ma7</th>\n",
       "      <th>exudate8</th>\n",
       "      <th>exudate9</th>\n",
       "      <th>exudate10</th>\n",
       "      <th>exudate11</th>\n",
       "      <th>exudate12</th>\n",
       "      <th>exudate13</th>\n",
       "      <th>exudate14</th>\n",
       "      <th>exudate15</th>\n",
       "      <th>euDist</th>\n",
       "      <th>diameter</th>\n",
       "      <th>amfm_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>49.895756</td>\n",
       "      <td>17.775994</td>\n",
       "      <td>5.270920</td>\n",
       "      <td>0.771761</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.486903</td>\n",
       "      <td>0.100025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>57.709936</td>\n",
       "      <td>23.799994</td>\n",
       "      <td>3.325423</td>\n",
       "      <td>0.234185</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>55.831441</td>\n",
       "      <td>27.993933</td>\n",
       "      <td>12.687485</td>\n",
       "      <td>4.852282</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>0.041817</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>0.128548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>40.467228</td>\n",
       "      <td>18.445954</td>\n",
       "      <td>9.118901</td>\n",
       "      <td>3.079428</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.272434</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.114790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>18.026254</td>\n",
       "      <td>8.570709</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475935</td>\n",
       "      <td>0.123572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>28.356400</td>\n",
       "      <td>6.935636</td>\n",
       "      <td>2.305771</td>\n",
       "      <td>0.323724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502831</td>\n",
       "      <td>0.126741</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>15.448398</td>\n",
       "      <td>9.113819</td>\n",
       "      <td>1.633493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541743</td>\n",
       "      <td>0.139575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.679649</td>\n",
       "      <td>9.497786</td>\n",
       "      <td>1.223660</td>\n",
       "      <td>0.150382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576318</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>66.691933</td>\n",
       "      <td>23.545543</td>\n",
       "      <td>6.151117</td>\n",
       "      <td>0.496372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500073</td>\n",
       "      <td>0.116793</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>22.141784</td>\n",
       "      <td>10.054384</td>\n",
       "      <td>0.874633</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560959</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7   exudate8   exudate9  \\\n",
       "0        1          1   22   22   22   19   18   14  49.895756  17.775994   \n",
       "1        1          1   24   24   22   18   16   13  57.709936  23.799994   \n",
       "2        1          1   62   60   59   54   47   33  55.831441  27.993933   \n",
       "3        1          1   55   53   53   50   43   31  40.467228  18.445954   \n",
       "4        1          1   44   44   44   41   39   27  18.026254   8.570709   \n",
       "5        1          1   44   43   41   41   37   29  28.356400   6.935636   \n",
       "6        1          0   29   29   29   27   25   16  15.448398   9.113819   \n",
       "7        1          1    6    6    6    6    2    1  20.679649   9.497786   \n",
       "8        1          1   22   21   18   15   13   10  66.691933  23.545543   \n",
       "9        1          1   79   75   73   71   64   47  22.141784  10.054384   \n",
       "\n",
       "   exudate10  exudate11  exudate12  exudate13  exudate14  exudate15    euDist  \\\n",
       "0   5.270920   0.771761   0.018632   0.006864   0.003923   0.003923  0.486903   \n",
       "1   3.325423   0.234185   0.003903   0.003903   0.003903   0.003903  0.520908   \n",
       "2  12.687485   4.852282   1.393889   0.373252   0.041817   0.007744  0.530904   \n",
       "3   9.118901   3.079428   0.840261   0.272434   0.007653   0.001531  0.483284   \n",
       "4   0.410381   0.000000   0.000000   0.000000   0.000000   0.000000  0.475935   \n",
       "5   2.305771   0.323724   0.000000   0.000000   0.000000   0.000000  0.502831   \n",
       "6   1.633493   0.000000   0.000000   0.000000   0.000000   0.000000  0.541743   \n",
       "7   1.223660   0.150382   0.000000   0.000000   0.000000   0.000000  0.576318   \n",
       "8   6.151117   0.496372   0.000000   0.000000   0.000000   0.000000  0.500073   \n",
       "9   0.874633   0.099780   0.023386   0.000000   0.000000   0.000000  0.560959   \n",
       "\n",
       "   diameter  amfm_class  label  \n",
       "0  0.100025           1      0  \n",
       "1  0.144414           0      0  \n",
       "2  0.128548           0      1  \n",
       "3  0.114790           0      0  \n",
       "4  0.123572           0      1  \n",
       "5  0.126741           0      1  \n",
       "6  0.139575           0      1  \n",
       "7  0.071071           1      0  \n",
       "8  0.116793           0      1  \n",
       "9  0.109134           0      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from csv file\n",
    "col_names = []\n",
    "for i in range(20):\n",
    "    if i == 0:\n",
    "        col_names.append('quality')\n",
    "    if i == 1:\n",
    "        col_names.append('prescreen')\n",
    "    if i >= 2 and i <= 7:\n",
    "        col_names.append('ma' + str(i))\n",
    "    if i >= 8 and i <= 15:\n",
    "        col_names.append('exudate' + str(i))\n",
    "    if i == 16:\n",
    "        col_names.append('euDist')\n",
    "    if i == 17:\n",
    "        col_names.append('diameter')\n",
    "    if i == 18:\n",
    "        col_names.append('amfm_class')\n",
    "    if i == 19:\n",
    "        col_names.append('label')\n",
    "\n",
    "data = pd.read_csv(\"messidor_features.txt\", names = col_names)\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Naive Bayes Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `sklearn.naive_bayes.GaussianNB` classifier. Use `sklearn.model_selection.cross_val_score` to do a 10-fold cross validation on the classifier. Display the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.61206897 0.56521739 0.63478261 0.53913043 0.6        0.66086957\n",
      " 0.56521739 0.54782609 0.60869565 0.64347826]\n",
      "Accuracy: 59.77286356821588\n"
     ]
    }
   ],
   "source": [
    "#Split Data into label vs not\n",
    "data_Y = data['label']\n",
    "\n",
    "data_Y\n",
    "data_X = data.drop(['label'], axis = 1)\n",
    "\n",
    "data_X\n",
    "\n",
    "# your code goes here\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(gnb,data_X,data_Y,cv=10) \n",
    "print(\"Scores:\", scores)   \n",
    "\n",
    "print(\"Accuracy:\", scores.mean()*100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the confusion matrix, precision, recall, and F1 score of your classifier.\n",
    "* `cross_val_score` returns the scores of every test fold. There is another function called `cross_val_predict` that returns predicted y values for every record in the test fold. In other words, for each element in the input, `cross_val_predict` returns the prediction that was obtained for that element when it was in the test set. Use `cross_val_predict` and `sklearn.metrics.confusion_matrix` to print the confusion matrix for the classifier.\n",
    "\n",
    "* `sckit-learn` also provides a useful function `sklearn.metrics.classification_report` for evaluating the classifier on a per-class basis. It is a text summary of the precision, recall, and F1 score for each class (support is just the actual class count). Display the classification report for your Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[502  38]\n",
      " [425 186]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.54      0.93      0.68       540\n",
      "     Class 1       0.83      0.30      0.45       611\n",
      "\n",
      "    accuracy                           0.60      1151\n",
      "   macro avg       0.69      0.62      0.56      1151\n",
      "weighted avg       0.69      0.60      0.56      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "#cross_val_predictestimator, X, y=None, groups=None, cv=’warn’, n_jobs=None, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’, method=’predict’\n",
    "y_pred = cross_val_predict(gnb,data_X,data_Y,cv=10)\n",
    "y_pred\n",
    "\n",
    "#Print out confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#data_Y is Y true \n",
    "print(confusion_matrix(data_Y,y_pred))\n",
    "\n",
    "#Text-Summary \n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Class 0','Class 1']\n",
    "print(classification_report(data_Y, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sklearn.metrics.roc_curve` plot a ROC curve for the Naive Bayes classifier. Also calculate the area under the curve (AUC) using `sklearn.metrics.roc_auc_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "YYY 159     1\n",
      "919     0\n",
      "817     0\n",
      "282     1\n",
      "911     0\n",
      "115     0\n",
      "672     0\n",
      "398     1\n",
      "1107    1\n",
      "1076    1\n",
      "706     1\n",
      "163     0\n",
      "450     1\n",
      "1128    1\n",
      "985     1\n",
      "1117    0\n",
      "560     1\n",
      "397     1\n",
      "299     0\n",
      "303     0\n",
      "94      1\n",
      "99      1\n",
      "1134    1\n",
      "491     0\n",
      "287     1\n",
      "437     1\n",
      "980     1\n",
      "106     1\n",
      "976     1\n",
      "293     0\n",
      "       ..\n",
      "1080    1\n",
      "71      0\n",
      "844     0\n",
      "97      0\n",
      "683     1\n",
      "663     1\n",
      "109     1\n",
      "1127    1\n",
      "695     0\n",
      "1028    0\n",
      "818     0\n",
      "84      1\n",
      "48      1\n",
      "335     0\n",
      "45      1\n",
      "525     0\n",
      "277     1\n",
      "1026    0\n",
      "613     0\n",
      "788     1\n",
      "263     1\n",
      "957     0\n",
      "189     1\n",
      "359     0\n",
      "913     1\n",
      "969     1\n",
      "425     0\n",
      "1089    0\n",
      "498     1\n",
      "1082    0\n",
      "Name: label, Length: 231, dtype: int64\n",
      "preds [4.36601777e-01 2.11318190e-01 3.34010387e-04 2.95019240e-02\n",
      " 2.48650621e-03 4.16774669e-04 4.19745100e-04 3.70306902e-04\n",
      " 3.50759119e-04 6.21324184e-03 1.00000000e+00 3.37358615e-04\n",
      " 1.00000000e+00 7.52887046e-04 1.78305175e-02 9.91117774e-04\n",
      " 9.99522352e-01 1.70272976e-01 3.22152036e-04 4.45963233e-04\n",
      " 1.00000000e+00 2.65364088e-04 1.00000000e+00 8.01336062e-04\n",
      " 7.38751730e-01 3.68469325e-04 3.40263975e-02 1.00000000e+00\n",
      " 5.59789984e-04 3.24357367e-03 5.48322453e-04 2.14918661e-02\n",
      " 6.26006768e-04 5.58367442e-01 9.45122577e-04 2.87260176e-03\n",
      " 1.05382253e-01 1.85308269e-03 1.51818772e-03 4.92947111e-04\n",
      " 3.54918332e-04 1.00000000e+00 1.02320635e-03 8.78799265e-01\n",
      " 3.89812500e-04 5.24370780e-04 3.52904673e-04 4.95867161e-04\n",
      " 5.56991876e-02 1.90892365e-01 7.83696231e-01 9.95317194e-01\n",
      " 3.78152566e-04 3.00371176e-01 4.51838860e-04 3.97006401e-04\n",
      " 4.71254866e-01 9.91988445e-03 3.30930384e-04 9.85641135e-01\n",
      " 3.88046196e-03 3.58504687e-03 5.76711215e-03 1.00000000e+00\n",
      " 3.10066782e-04 7.11438834e-02 8.97233450e-02 7.49349037e-01\n",
      " 5.00795495e-02 8.66355584e-03 1.00000000e+00 1.61575920e-03\n",
      " 5.85438650e-03 2.77222273e-03 3.99012380e-04 7.75291031e-04\n",
      " 2.56056921e-03 3.22374652e-04 3.83809478e-04 3.25305564e-04\n",
      " 4.97718594e-04 2.26026662e-01 9.03301588e-01 3.66919369e-02\n",
      " 6.35784801e-04 3.06600542e-04 8.80806006e-03 7.09868137e-03\n",
      " 1.00000000e+00 5.44800107e-04 6.09660110e-04 1.01683966e-02\n",
      " 3.83328048e-04 5.22576958e-04 3.88207953e-04 1.12852439e-02\n",
      " 4.83050920e-04 1.73342784e-02 9.15714804e-03 9.99086413e-01\n",
      " 1.00000000e+00 2.13432456e-03 3.79464087e-04 3.66252699e-02\n",
      " 3.74559740e-01 1.40142324e-01 4.29947402e-04 1.35421157e-03\n",
      " 9.99354916e-01 6.15666529e-04 3.59935019e-04 2.74161610e-03\n",
      " 2.04894182e-03 2.26888485e-03 4.60041339e-04 4.88833735e-02\n",
      " 4.17248577e-04 2.24570133e-01 5.48358416e-01 3.14678550e-04\n",
      " 3.21613345e-04 7.47099863e-02 9.89139390e-01 9.99894594e-01\n",
      " 5.66193838e-04 4.80186298e-04 1.54727501e-01 1.00000000e+00\n",
      " 1.66422047e-02 6.94370179e-03 6.51750657e-03 2.08969148e-02\n",
      " 4.30746983e-01 7.00776245e-04 5.27264125e-04 1.37322954e-02\n",
      " 3.05275837e-02 7.71713496e-01 4.59368552e-04 7.93325895e-04\n",
      " 2.82703806e-01 7.18853932e-04 1.71225861e-03 6.48773327e-01\n",
      " 1.36429254e-01 4.65252684e-04 8.14185193e-01 1.00000000e+00\n",
      " 3.93887719e-04 5.92583716e-04 4.94302509e-04 5.52529463e-04\n",
      " 9.94729783e-01 4.85746573e-04 4.82336750e-02 3.92888511e-03\n",
      " 1.53668505e-02 3.69598578e-03 2.79876463e-01 9.90239934e-03\n",
      " 9.97328528e-01 2.59789715e-02 1.13646917e-03 5.01903048e-04\n",
      " 5.54933051e-04 4.77323821e-04 4.58656791e-04 4.93217137e-04\n",
      " 1.00000000e+00 1.00000000e+00 1.10141680e-03 2.45235381e-03\n",
      " 6.75618950e-03 6.31481662e-03 6.47280256e-04 2.38733878e-02\n",
      " 1.00000000e+00 1.62637672e-02 8.81468003e-03 7.89195217e-04\n",
      " 1.00000000e+00 1.82993470e-01 1.17695416e-03 6.90290908e-03\n",
      " 9.97355150e-01 1.00000000e+00 1.01578829e-01 4.94814113e-04\n",
      " 6.93867131e-03 2.15932088e-02 6.33151554e-03 2.81765573e-02\n",
      " 1.04338390e-02 7.55932309e-01 2.74105697e-03 4.34390420e-04\n",
      " 1.00000000e+00 4.37403122e-03 3.14511485e-04 1.00000000e+00\n",
      " 9.94776795e-01 1.00000000e+00 4.57098319e-04 2.69400597e-03\n",
      " 9.17160654e-04 9.99999982e-01 1.20880686e-02 8.31325638e-03\n",
      " 2.22071191e-01 3.98825129e-04 4.39051203e-04 3.21364782e-04\n",
      " 8.87716748e-04 8.10265691e-01 3.14698982e-04 1.95287129e-03\n",
      " 3.95682223e-04 2.14573732e-01 3.16473025e-04 2.28721911e-02\n",
      " 7.17575361e-01 5.45691653e-04 3.14133529e-02 1.03604448e-02\n",
      " 3.06274644e-02 4.47557782e-01 9.28053678e-04 1.49787729e-03\n",
      " 3.78381886e-03 9.99983946e-01 3.89160340e-04]\n",
      "AUC SCORE 0.7442498095963442\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code goes here\n",
    "#Split code\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.20, random_state=None)\n",
    "\n",
    "#Fit the Naive Bayes Classifier to the training data by calling the fit method\n",
    "gnb = gnb.fit(X_train,Y_train)\n",
    "print(gnb)\n",
    "\n",
    "#Predict_Proba\n",
    "from sklearn import metrics\n",
    "prob_array = gnb.predict_proba(X_test)\n",
    "#len is 231\n",
    "preds= prob_array[:,1]\n",
    "\n",
    "#Create the ROC Curve\n",
    "fpr, tpr,thresholds = metrics.roc_curve(Y_test,preds)\n",
    "\n",
    "print(\"YYY\", Y_test)\n",
    "print(\"preds\", preds)\n",
    "\n",
    "#Print the AUC\n",
    "from sklearn.metrics import roc_auc_score \n",
    "print(\"AUC SCORE\", roc_auc_score(Y_test, preds))\n",
    "\n",
    "\n",
    "# Do not change this code! This plots the ROC curve.\n",
    "# Just replace the fpr and tpr above with the values from your roc_curve\n",
    "plt.plot([0,1],[0,1],'k--') #plot the diagonal line\n",
    "plt.plot(fpr, tpr, label='NB') #plot the ROC curve\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('ROC Curve Naive Bayes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Neighbor (KNN) Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you did not scale the data prior to runnng Naive Bayes. But it is critical to scale the data before running Nearest Neighbor. Explain why we don't need to scale the data for NB, but do need to for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Your answer here as a comment.\n",
    "Naive Bayes already sets priors on the data we feed. KNN uses Euclidean distance which is sensitive to \n",
    "the differences in distances so we need to scale it so all feaures are weighed equally.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `sklearn.preprocessing.MinMaxScaler` to normalize the dataset’s features from [0,1]. Use the normalized dataset moving forward. Note that MinMaxScaler returns a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 0.14      , ..., 0.53080127, 0.26113347,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 0.15333333, ..., 0.68230157, 0.5363407 ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 0.40666667, ..., 0.72683611, 0.43797313,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [1.        , 0.        , 0.32      , ..., 0.85928137, 0.446002  ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 0.25333333, ..., 0.52665345, 0.30245578,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 0.04      , ..., 0.83950012, 0.192513  ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# your code goes here\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "normalized_data_X= min_max_scaler.fit_transform(data_X)\n",
    "normalized_data_X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use a `sklearn.neighbors.KNeighborsClassifier` with `k = 5` to classify the normalized data. Use a 10-fold CV to display precision, recall and accuracy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.62068966 0.70434783 0.64347826 0.66086957 0.66086957 0.69565217\n",
      " 0.55652174 0.62608696 0.53913043 0.64347826]\n",
      "Accuracy: 63.51124437781108\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.60      0.65      0.62       540\n",
      "     Class 1       0.67      0.62      0.64       611\n",
      "\n",
      "    accuracy                           0.64      1151\n",
      "   macro avg       0.64      0.64      0.63      1151\n",
      "weighted avg       0.64      0.64      0.64      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "#need to access with the array- for x and y\n",
    "model= neigh.fit(normalized_data_X, data_Y)\n",
    "\n",
    "\n",
    "#Scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(neigh,normalized_data_X,data_Y,cv=10) \n",
    "print(\"Scores:\", scores)   \n",
    "\n",
    "print(\"Accuracy:\", scores.mean()*100)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "#cross_val_predictestimator, X, y=None, groups=None, cv=’warn’, n_jobs=None, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’, method=’predict’\n",
    "y_pred = cross_val_predict(model,normalized_data_X,data_Y,cv=10)\n",
    "y_pred\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Class 0','Class 1']\n",
    "print(classification_report(data_Y, y_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `sklearn.model_selection.GridSearchCV` to find the best value of k for this data. Try k values from 1-30. Display the best value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 23}\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]}\n",
    "knn = KNeighborsClassifier()\n",
    "gridSearch = GridSearchCV(knn, parameters, cv= 10)\n",
    "gridSearch.fit(normalized_data_X, data_Y) \n",
    "print(gridSearch.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the accuracy, precision, and recall of a KNN classifier using the value of k that you just found. (Note that the values are improved because you're using the optimal k for this data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.62      0.72      0.67       540\n",
      "     Class 1       0.71      0.61      0.65       611\n",
      "\n",
      "    accuracy                           0.66      1151\n",
      "   macro avg       0.66      0.66      0.66      1151\n",
      "weighted avg       0.67      0.66      0.66      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "#Run model with best \n",
    "neigh = KNeighborsClassifier(n_neighbors=23)\n",
    "#need to access with the array- for x and y\n",
    "model= neigh.fit(normalized_data_X, data_Y)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "#cross_val_predictestimator, X, y=None, groups=None, cv=’warn’, n_jobs=None, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’, method=’predict’\n",
    "y_pred = cross_val_predict(model,normalized_data_X,data_Y,cv=10)\n",
    "y_pred\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Class 0','Class 1']\n",
    "print(classification_report(data_Y, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wrap the whole process in another cross-validation to report the final accuarcy of your KNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[390 150]\n",
      " [241 370]]\n",
      "0.6602953953084274\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.72      0.67       540\n",
      "           1       0.71      0.61      0.65       611\n",
      "\n",
      "    accuracy                           0.66      1151\n",
      "   macro avg       0.66      0.66      0.66      1151\n",
      "weighted avg       0.67      0.66      0.66      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from sklearn.metrics import accuracy_score\n",
    "cross_val_predict(gridSearch,normalized_data_X, data_Y, cv= 10)\n",
    "\n",
    "print(confusion_matrix(data_Y,y_pred))\n",
    "nested_accuracy = accuracy_score(data_Y,y_pred)\n",
    "print(nested_accuracy)\n",
    "print(classification_report(data_Y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed why dimensionality reduction is critical to KNN because of the curse of dimensionality. So we may want to perform a dimensionality reduction with PCA before running KNN. (Remember that you can also reduce dimensionality by performing feature selection and feature engineering.) \n",
    "\n",
    "An important note about PCA is that is should only be performed on the **training** data, then you transform the test data into the PCA space that was found on the training data. \n",
    "\n",
    "So when you are doing cross-validation, the PCA needs to happen *inside of your CV loop*. This way, it is performed on the training set for the first fold, then the test set is put into that space. On the second fold, it is performed on the trainng set for the second fold, and the test set is put into that space. And so on for the remaining folds. \n",
    "\n",
    "In order to do this with Python, you must create what's called a `Pipeline` and pass that in to the cross validation. This is a very important concept for Data Mining and Machine Learning, so let's practice it here.\n",
    "\n",
    "We have provided some of the necessary code for you, but this code is not complete. You need to finish it by doing the following:\n",
    "* pass the pipeline and the parameters into a `GridSearchCV` with a 5-fold cross validation\n",
    "* call `fit()` on the GridSearchCV and pass in the normalized data (X_values, Y_values)\n",
    "* print out the `best_score_` and `best_params_` from the GridSearchCV\n",
    "\n",
    "This will show you the best number of principal components to keep (number of dimensions) and the best value of k to use (number of neighbors).\n",
    "\n",
    "[Then you'd want to wrap this GridSearchCV in another cross-validation to do a nested cross-validation and get an accuracy estimate. But we'll leave that for another time... :) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__n_neighbors': 21, 'pca__n_components': 8}\n",
      "0.6602953953084274\n"
     ]
    }
   ],
   "source": [
    "# Define a pipeline to search for the best combination of PCA truncation and n_neighbors.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#create a PCA\n",
    "pca = PCA()\n",
    "\n",
    "#create a KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#create a pipeline that does a PCA and a KNN\n",
    "pipe = Pipeline(steps=[('pca', pca), ('knn', knn)])\n",
    "\n",
    "#Set up the parameters you want to tune for each of your pipeline steps\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(1, 19)), #find how many principal componenet to keep\n",
    "    'knn__n_neighbors': list(range(1, 30)),  #find the best value of k\n",
    "}\n",
    "\n",
    "# your code goes here:\n",
    "clf = GridSearchCV(pipe,param_grid,cv = 5)\n",
    "# pass the pipeline and the parameters into a GridSearchCV with a 5-fold cross validation\n",
    "# call fit() on the GridSearchCV and pass in the normalized data (X_values, Y_values)\n",
    "clf.fit(normalized_data_X,data_Y)\n",
    "# print out the best_score_ and best_params_ from the GridSearchCV\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
